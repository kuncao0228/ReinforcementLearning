{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "import gym\n",
    "import network_sim\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(30, 64)\n",
    "        self.fc2 = nn.Linear(64, 256)\n",
    "        self.fc_mu = nn.Linear(256, 1)\n",
    "        self.fc_std = nn.Linear(256, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mu = 2 * self.tanh(self.fc_mu(x))\n",
    "        std = self.softplus(self.fc_std(x)) + 1e-3\n",
    "        return mu, std\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            mu, std = self.forward(state)\n",
    "            n = Normal(mu, std)\n",
    "            action = n.sample()\n",
    "        \n",
    "        return np.clip(action.item(), -1e12, 1e12)\n",
    "            \n",
    "        \n",
    "#         return np.clip(action.item(), -2., 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(30, 64)\n",
    "        self.fc2 = nn.Linear(64, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, memory_size: int) -> None:\n",
    "        self.memory_size = memory_size\n",
    "        self.buffer = deque(maxlen=self.memory_size)\n",
    "\n",
    "    def add(self, experience) -> None:\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size: int, continuous: bool = True):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        if continuous:\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            return [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "        else:\n",
    "            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "            return [self.buffer[i] for i in indexes]\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History length: 10\n",
      "Features: ['sent latency inflation', 'latency ratio', 'send ratio']\n",
      "Getting min obs for ['sent latency inflation', 'latency ratio', 'send ratio']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kun/anaconda3/envs/CVpy3/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PccNs-v0')\n",
    "policy = PolicyNetwork().to(device)\n",
    "value = ValueNetwork().to(device)\n",
    "optim = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
    "value_optim = torch.optim.Adam(value.parameters(), lr=1e-3)\n",
    "gamma = 0.99\n",
    "writer = SummaryWriter('a2c_logs_pen')\n",
    "memory = Memory(32)\n",
    "batch_size = 32\n",
    "is_learn = False\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.00, Ewma Reward: 0.00\n",
      "Epoch:0, episode reward is 146.8920976195483\n",
      "Reward: 146.89, Ewma Reward: 1.47\n",
      "Reward: -4.06, Ewma Reward: 1.41\n",
      "Reward: 26.46, Ewma Reward: 1.66\n",
      "Reward: -28.47, Ewma Reward: 1.36\n",
      "Reward: -36.27, Ewma Reward: 0.99\n",
      "Reward: -72.05, Ewma Reward: 0.26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c1a1008283e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PCC-RL/src/gym/network_sim.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_cwnd_delta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m#print(\"Running for %fs\" % self.run_dur)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_for_dur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_dur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msender\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0msender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PCC-RL/src/gym/network_sim.py\u001b[0m in \u001b[0;36mrun_for_dur\u001b[0;34m(self, dur)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mevent_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_hop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_latency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheappop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;31m#print(\"Got event %s, to link %d, latency %f at time %f\" % (event_type, next_hop, cur_latency, event_time))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in count():\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    k = 0\n",
    "    for time_steps in range(200):\n",
    "        k += 1\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = policy.select_action(state_tensor)\n",
    "        next_state, reward, done, _ = env.step([action])\n",
    "        episode_reward += reward\n",
    "\n",
    "        reward = (reward + 8.1) / 8.1\n",
    "        memory.add((state, next_state, action, reward, done))\n",
    "        if k == 32 or time_steps == 199:\n",
    "            k = 0\n",
    "            steps += 1\n",
    "            experiences = memory.sample(batch_size)\n",
    "            batch_state, batch_next_state, batch_action, batch_reward, batch_done = zip(*experiences)\n",
    "            batch_state = torch.FloatTensor(batch_state).to(device)\n",
    "            batch_next_state = torch.FloatTensor(batch_next_state).to(device)\n",
    "            batch_action = torch.FloatTensor(batch_action).unsqueeze(1).to(device)\n",
    "            batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(device)\n",
    "            batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(device)\n",
    "\n",
    "            # print(batch_state.shape, batch_next_state.shape, batch_action.shape, batch_reward.shape)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                value_target = batch_reward + gamma * (1 - batch_done) * value(batch_next_state)\n",
    "                advantage = value_target - value(batch_state)\n",
    "            mu, std = policy(batch_state)\n",
    "            # print(prob.shape)\n",
    "            n = Normal(mu, std)\n",
    "            log_prob = n.log_prob(batch_action)\n",
    "            loss = - log_prob * advantage\n",
    "            # print(loss.shape)\n",
    "            loss = loss.mean()\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            writer.add_scalar('action loss', loss.item(), steps)\n",
    "\n",
    "            value_loss = F.mse_loss(value_target, value(batch_state))\n",
    "            value_optim.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optim.step()\n",
    "            writer.add_scalar('value loss', value_loss.item(), steps)\n",
    "\n",
    "            memory.clear()\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    writer.add_scalar('episode reward', episode_reward, epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:{}, episode reward is {}'.format(epoch, episode_reward))\n",
    "        torch.save(policy.state_dict(), 'a2c-pen-policy.para')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVpy3",
   "language": "python",
   "name": "dlwp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
